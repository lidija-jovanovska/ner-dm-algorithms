<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EFFICIENT CODEBOOK AND FACTORIZATION FOR SECOND ORDER REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">EFFICIENT CODEBOOK AND FACTORIZATION FOR SECOND ORDER REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2022-03-08T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning rich and compact representations is an open topic in many fields such as word embedding, visual question-answering, object recognition or image retrieval. Although deep neural networks (convolutional or not) have made a major breakthrough during the last few years by providing hierarchical, semantic and abstract representations for all of these tasks, these representations are not necessary as rich as needed nor as compact as expected. Models using higher order statistics, such as bilinear pooling, provide richer representations at the cost of higher dimensional features. Factorization schemes have been proposed but without being able to reach the original compactness of first order models, or at a heavy loss in performances. This paper addresses these two points by extending factorization schemes to codebook strategies, allowing compact representations with the same dimensionality as first order representations, but with second order performances. Moreover, we extend this framework with a joint codebook and factorization scheme, granting a reduction both in terms of parameters and computation cost. This formulation leads to state-of-the-art results and compact second-order models with few additional parameters and intermediate representations with a dimension similar to that of first-order statistics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Learning rich and compact representations is an open topic in many fields such as word embedding <ref type="bibr" target="#b16">(Mikolov et al. (2013)</ref>), visual question-answering <ref type="bibr" target="#b5">(Yang et al. (2016)</ref>), object recognition <ref type="bibr" target="#b23">(Szegedy et al. (2015)</ref>) or image retrieval <ref type="bibr" target="#b19">(Opitz et al. (2017)</ref>). The standard approach extracts features from the input data <ref type="bibr">(text, image, etc.)</ref> and builds a representation that will be next processed for a given task <ref type="bibr">(classification, retrieval, etc.)</ref>. These features are usually extracted with deep neural networks and the representation is trained in an end-to-end manner. Recently, representations that compute first order statistics over input data have been outperformed by improved models that compute higher order statistics such as bilinear models. This embedding strategy generates richer representations and has been applied in a wide range of tasks : word embedding <ref type="bibr" target="#b2">(Clinchant &amp; Perronnin (2013)</ref>), VQA <ref type="bibr" target="#b9">(Kim et al. (2017)</ref>), fine grained classification <ref type="bibr" target="#b28">(Wei et al. (2018)</ref>), etc. and gets state-of-the-art results. For instance, Bilinear models perform the best for fine grained visual classification tasks by producing efficient representations that model more details within an image than classical first order statistics <ref type="bibr" target="#b14">(Lin et al. (2015)</ref>).</p><p>However, even if the increase in performances is unquestionable, second order models suffer from a collection of drawbacks: Their intermediate dimension increases quadratically with respect to input features dimension, they require a projection to lower dimension that is costly both in number of parameters and in computation, they are harder to train than first order models due to the increased dimension, they lack a proper adapted pooling scheme which leads to sub-optimal representations.</p><p>The two main downsides, namely the high dimensional output representations and the sub-efficient pooling scheme, have been widely studied over the last decade. On one hand, the dimensionality issue has been studied through factorization scheme, either representation oriented such as Compact Bilinear Pooling ) and Hadamard Product for Low Rank Bilinear Pooling <ref type="bibr" target="#b9">(Kim et al. (2017)</ref>), or task oriented as Low-rank Bilinear Pooling <ref type="bibr" target="#b10">(Kong &amp; Fowlkes (2017)</ref>). While these factorization schemes are efficient in term of computation cost and number of parameters, the intermediate representation is still too large (typically 10k dimension) to ease the training process and using lower dimension greatly deteriorate performances.</p><p>On the other hand, it is well-known that global average pooling schemes aggregate unrelated features. This problem has been tackled by the use of codebooks such as VLAD <ref type="bibr" target="#b0">(Arandjelovic &amp; Zisserman (2013)</ref>) or, in the case of second-order information, Fisher Vectors <ref type="bibr" target="#b20">(Perronnin et al. (2010)</ref>). These strategies have been enhanced to be trainable in an end-to-end manner <ref type="bibr" target="#b1">(Arandjelovic et al. (2016)</ref>; <ref type="bibr" target="#b24">Tang et al. (2016)</ref>). However, using a codebook on end-to-end trainable second order features leads to an unreasonably large model, since the already large second order model has to be duplicated for each entry of the codebook. This is for example the case in MFAFVNet <ref type="bibr" target="#b13">(Li et al. (2017b)</ref>) for which the second order layer alone (i.e., without the CNN part) already costs over 25M parameters and 40 GFLOP, or about as much as an entire ResNet50.</p><p>In this paper, we tackle both of these shortcomings (intermediate representation cost and lack of proper pooling) by exploring joint factorization and codebook strategies. Our main results are the following:</p><p>-We first show that state-of-the-art factorization schemes can already be improved by the use of a codebook pooling, albeit at a prohibitive cost. -We then propose our main contribution, a joint codebook and factorization scheme that achieves similar results at a much reduced cost.</p><p>Since our approach focuses on representation learning and is task agnostic, we validate it in a retrieval context on several image datasets to show the relevance of the learned representations. We show our model achieves competitive results on these datasets at a very reasonable cost.</p><p>The remaining of this paper is organized as follows: in the next section, we present the related work on second order pooling, factorization schemes and codebook strategies. In section 3, we present our factorization with the codebook strategy and how we improve its integration. In section 4, we show an ablation study on the Stanford Online Products dataset (Oh <ref type="bibr" target="#b18">Song et al. (2016)</ref>). Finally, we compare our approach to the state-of-the-art methods on three image retrieval datasets <ref type="bibr">(Stanford Online Products, CUB-200-2001, Cars-196)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we focus on methods that use representations based on second-order information and we provide a comparison in terms of computational efficiency and number of parameters for the second-order layer. These second-order methods exploit either bilinear pooling (section 2.1) and factorization schemes (section 2.2) or codebook strategies (section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SECOND-ORDER POOLING</head><p>In this section, we briefly review end-to-end trainable Bilinear pooling <ref type="bibr" target="#b14">(Lin et al. (2015)</ref>). This method extracts representations from the same image with two CNNs and computes the crosscovariance as representation. This representation outperforms its first-order version and other second-order representations such as Fisher Vectors <ref type="bibr" target="#b20">(Perronnin et al. (2010)</ref>) once the global architecture is fine-tuned. However, bilinear pooling leads to a small improvement compared to secondorder pooling (i.e., the covariance of the CNN features) at the cost of a higher computation. Most of recent works on bilinear pooling only focus on computing covariance of the extracted features, that is :</p><formula xml:id="formula_0">y = i∈S x i x T i = XX T ∈ R d×d (1)</formula><p>where X = {x i ∈ R d |i ∈ S} ∈ R d×hw is the matrix of concatenated CNN features, h and w are the height and the width of the extracted feature map. Another formulation is the vectorized version of y obtained by computing the Kronecker product (⊗) of x i with itself:</p><formula xml:id="formula_1">y = i∈S x i ⊗ x i = vec(XX T ) ∈ R d 2 (2)</formula><p>Due to the very high dimension of this representation, most recent works on bilinear pooling tend to improve this representation by providing new factorization scheme to reduce the computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">FACTORIZATION SCHEMES</head><p>Recent works on bilinear pooling proposed factorization schemes with two objectives: avoiding the direct computation of second order features and reducing the high dimensionality output representation.  proposed Compact Bilinear Pooling that tackles the high dimensionality of second-order features by analyzing two low-rank approximations of the equivalent polynomial kernel (equation (2) from ):</p><formula xml:id="formula_2">B(X ) ; B(Y) = xs∈B(X ) yu∈B(Y) x s ; y u 2 ≈ xs∈B(X ) φ(x s ) ; yu∈B(Y) φ(y u )<label>(3)</label></formula><p>with φ the mapping function that approximates the kernel. This compact formulation allows to keep less than 4% of the components with nearly no loss in performances compared to the uncompressed model. <ref type="bibr" target="#b9">Kim et al. (2017)</ref> improved Compact Bilinear Pooling using Hadamard Product and generalized it for visual question-answering tasks. <ref type="bibr" target="#b10">Kong &amp; Fowlkes (2017)</ref> introduced Low Rank Bilinear Pooling (LR-BP) that takes advantage of SVM formulation by jointly training the network and the classifier. The authors propose an efficient factorization as they never compute directly the covariance features and have slightly better results compared to the uncompressed bilinear pooling or Compact Bilinear Pooling. However, as it is, their method is limited to classification with the SVM formulation and cannot be used for other tasks. <ref type="bibr" target="#b12">Li et al. (2017a)</ref> introduced Factorized Bilinear Network (FBN), an improved version of Compact Bilinear Pooling. FBN never directly computes the covariance matrix. Instead, it projects the features using a hyperplan and computes a quadratic form. The matrix of this quadratic form is supposed rank deficient to reduce the number of parameters and the computation cost with negligible loss in performances. Thus, the output representation (or directly the number of classes for classification tasks) is generated by concatenating these scalars for all projections. <ref type="bibr" target="#b28">Wei et al. (2018)</ref> presented Grassmann Bilinear Pooling as a new factorization scheme. The objective is to take advantage of the rank deficient covariance matrix using Singular Value Decomposition (SVD) and then compute the classifier over these Grassmann manifolds. This factorization is efficient in the sense that it never directly computes the second-order representation and contrary to LR-BP, this formulation allows the construction of a representation, by replacing the number of classes by the representation dimension. In practice, however, they need to greatly reduce the input feature dimension due to the SVD complexity which is cubic in the feature dimension.</p><p>In this work, we start from a similar factorization as <ref type="bibr" target="#b9">Kim et al. (2017)</ref> detailed in section 3.1. However, this factorization is improved by the introduction of a codebook strategy that allows smaller representation dimension and improves performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CODEBOOK STRATEGIES</head><p>An acknowledged drawback of pooling methods is that they pool unrelated features that may decrease performances. To cope with this observation, codebook strategies have been proposed and greatly improved performances by pooling only features that belong to the same codeword.</p><p>The first representations that take advantage of codebook strategies are Bag of Words (BoW) and in the case of second order information Fisher Vectors <ref type="bibr" target="#b20">(Perronnin et al. (2010)</ref>). Fisher Vectors (FVs) extend the BoW framework by replacing the hard assignment of BoW by a Gaussian Mixture Model (GMM) and then compute the representation as an extension of the Fisher Kernel. In practice, covariance matrices are supposed to be diagonal which leads to representations of size N (2d + 1) where d is the dimension of the features and N is the codebook size. <ref type="bibr" target="#b24">Tang et al. (2016)</ref> proposed FisherNet, an architecture that integrates FVs as differentiable layer. The proposed layer outperforms non-trainable FVs approach but nonetheless has the high output dimension of the original FV. <ref type="bibr" target="#b13">Li et al. (2017b)</ref> introduced MFA-FV network, a deep architecture which extends the MFA-FV of <ref type="bibr" target="#b4">Dixit &amp; Vasconcelos (2016)</ref> by producing a second order information embedding trainable in an endto-end manner. The proposed formulation takes advantage of both worlds: MFA-FV generates an efficient representation of non-linear manifolds with a small latent space and it can be trained in an end-to-end manner. The main drawbacks of their method is the direct computation of second-order features for each codeword (computation cost), the raw projection of this covariance matrix into the latent space for each codeword (computation cost and number of parameters), and finally the representation dimension. In the original paper, the proposed representation reaches 500k dimension, twice the already high dimension of Bilinear Pooling.</p><formula xml:id="formula_3">Method C. F. #param computation dim. BP × × - hwd 2 [206M] d 2 [262k] CBP-RM † × 2dD [10M] 2hwdD [8G] D [10k] CBP-TS † × 2d [1k] hw(d + 3D log D) [94M] D [10k] HPBP × 2dD 2hwdD D FBN × dkD [5M] hwdkD [4.1G] D [512] Grassmann BP × LdD [4.2M] c 3 + DLc 2 [2.3G] D [512] MFAFVNet † × N (d 2 + dL) [27M] N d 2 (2P + L) [42G] N DL [500k] Ours 2RdD [4.2M] 2hwdDR [3.2G] D [512]</formula><p>For a more compact review, computation cost, number of parameters, use of codebook and/or factorization are sumed-up in table 1. This table shows that, to our knowledge, no efficient factorization combined with codebook strategy has been proposed to exploit the richer representation due to codebook but at a small increase in terms of number of parameters or computation cost. As is shown in this table, our proposition combine the best of both worlds by providing a joint codebook and factorization optimization scheme with a similar number of parameters and computation cost to that of methods without codebook strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD OVERVIEW</head><p>In section 3.1, we detail the initial factorization scheme and the properties of the Kronecker Product and the dot product that are used in the two next sections. In section 3.2, we extend this factorization to a codebook strategy and show the limitations of this architecture in terms of computation cost, low-rank approximation, number of parameters, etc. In section 3.3 we enhance this representation by sharing projectors to all codewords into the codebook, leading to a joint codebook and factorization optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">INITIAL FACTORIZATION SCHEME</head><p>In this section, we present the factorization used and highlight the advantages and limitations of this scheme. For a given input feature x ∈ R d , we compute its second-order representation x ⊗ x ∈ R d 2 and project it into a smaller subspace with W ∈ R d 2 ×D to build the output feature z(x) ∈ R D . These output features are then pooled to build the output representation z:</p><formula xml:id="formula_4">z = x z(x) = x W T (x ⊗ x)<label>(4)</label></formula><p>In the rest of the paper, we use the notation z i that refers to the i-th dimension of the output representation z and z i (x) the i-th dimension of the output feature z(x), that is:</p><formula xml:id="formula_5">z i = x z i (x) = x w T i (x ⊗ x) = x w i ; x ⊗ x (5)</formula><p>with w i ∈ R d 2 a column of W and • ; • the dot product. Then we enforce a factorization of w i to take advantage of the properties of dot product and Kronecker product, that is ∀(a, b, c, d</p><formula xml:id="formula_6">) ∈ (R d ) 4 , a ⊗ c ; b ⊗ d = a ; b c ; d .</formula><p>Thus, we use the following rank one decomposition of w i = u i ⊗ v i where (u, v) ∈ (R d ) 2 . z i (x) from equation (5) becomes:</p><formula xml:id="formula_7">z i (x) = u i ; x v i ; x (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>This factorization is efficient in term of parameters as it needs only 2dD parameters instead of d 2 D for the full projection matrix. However, even if this rank one decomposition allows interesting dimension reduction ; <ref type="bibr" target="#b9">Kim et al. (2017)</ref>) it is not enough to keep rich representation with smaller dimension. Consequently, we extend the second-order feature to a codebook strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CODEBOOK STRATEGY</head><p>To extend second-order pooling, we want to pool only similar features, that is which belong to the same codeword. This codebook pooling is interesting because each projection to a sub-space should have only similar features, and they should be encoded with fewer dimension. For a codebook size of N , we compute an assignment function h(•) ∈ R N . This function could be a hard assignment function (e.g., arg min over distance to each cluster) or a soft assignment (e.g., the softmax function). Thus, our output feature z i (x) becomes:</p><formula xml:id="formula_9">z i (x) = w i ; h(x) ⊗ x ⊗ h(x) ⊗ x (7) Remark that now W ∈ R N 2 d 2 ×D and w i ∈ R N 2 d 2 .</formula><p>Here, we duplicate h(x) for generalization purpose: In the case of the original bilinear pooling this formulation becomes h 1 (x 1 ) ⊗ x 1 ⊗ h 2 (x 2 ) ⊗ x 2 and we can use two different codebooks, one for each network. Moreover, this formulation allows more degrees of freedom for the next factorization. As in equation 6, we enforce the rank one decomposition of w i = p i ⊗ q i where (p i , q i ) ∈ (R N d ) 2 . This first factorization leads to the following output feature z i (x):</p><formula xml:id="formula_10">z i (x) = p i ; h(x) ⊗ x q i ; h(x) ⊗ x (8)</formula><p>This intermediate representation is too large to be computed directly, e.g. using N = 100 and the same parameters as in Table <ref type="table" target="#tab_0">1</ref>, we have intermediate features with 50k dimensions and the two intermediate feature maps consume about 320MB of memory which becomes rapidly intractable if the dimension of z is above 10. Then, we enforce two other factorizations of p i = j e (j) ⊗ u i,j and q i = j e (j) ⊗ v i,j where e (j) ∈ R N is the j-th vector from the natural basis of R N and (u i,j , v i,j ) ∈ (R d ) 2 . Then equation (8) becomes:</p><formula xml:id="formula_11">z i (x) =   N j=1 h(x) ; e (j) x ; u i,j     N j=1 h(x) ; e (j) x ; v i,j   (9)</formula><p>The decompositions of p i and q i play similar roles as intra-projection in VLAD representation <ref type="bibr" target="#b3">(Delhumeau et al. (2013)</ref>). Indeed, if we consider h(•) as a hard assignment function, the projection that will be computed is the only one assigned to the corresponding codewords. Thus, this model learns a projection matrix for each codebook entry.</p><p>Furthermore, equation ( <ref type="formula">9</ref>) can be factorized using h j (x), the j-th component of h(x):</p><formula xml:id="formula_12">z i (x) = N j=1 h j (x) x ; u i,j N j=1 h j (x) x ; v i,j = h(x) T U T i x h(x) T V T i x (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where U i ∈ R d×N is the matrix concatenating the projections of all entries of the codebook for the i-th output dimension. We call this approach C-CBP as it corresponds to the extension of CBP ) to a Codebook strategy.</p><p>This representation has multiple advantages: First, it computes second order features that leads to better performances compared to its first order counterpart. Second, the first factorization provides an efficient alternative in terms of number of parameters and computation despite the decreasing performances when it reaches small representation dimension. This downside is addressed by the third advantage which is the codebook strategy. It allows the pooling of only related features while their projections to a sub-space is more compressible. However, even if this codebook strategy improves the performances, the number of parameters is in O(dDN ) As such, using large codebook may become intractable. In the next section, we extend this scheme by sharing a set of projectors and enhance the decompositions of p i and q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SHARING PROJECTORS</head><p>In the previous model, for a given codebook entry, there is one dedicated projector that is learned to map to a smaller vector space all features that belong to this codebook entry. The proposed idea is, instead of using a a one-to-one correspondence, we learn a set of projectors that is shared across the codebook. The reasoning behind is that projectors from different codebook entries are unlikely to be all orthogonal. By doing such hypothesis, that is, the vector space spaned by the projection matrices has a lower dimension than the codebook itself, we can have smaller models with nearly no loss in performances. To check this hypothesis, we extend the proposed factorization from section 3.2. We want to generate U i from { U i } i∈{1,...,R} and V i from { V i } i∈{1,...,R} where R is the number of projections in the set. Then the two new enforced factorization of p i and q i are:</p><formula xml:id="formula_14">p i (x) = r f p,r h(x) e (r) ⊗ u i,r and q i (x) = r f q,r h(x) e (r) ⊗ v i,r<label>(11)</label></formula><p>where f p and f q are two functions from R N to R R that transform the codebook assignment into a set of coefficient which generate their respective projection matrices. Then, using these factorizations lead to the following equation:</p><formula xml:id="formula_15">z i (x) = R r=1 f p,r h(x) x ; u i,j R r=1 f q,r h(x) x ; v i,j = f p h(x) T U T i x f q h(x) T V T i x<label>(12)</label></formula><p>In this paper, we only study the case of a linear projection to the sub-space R R , that is f p : a → f p (a) = A T a and f q : a → f q (a) = B T a with (A, B) ∈ (R N ×R ) 2 . Finally, the fully factorized z transform is computed using the following equation:</p><formula xml:id="formula_16">z i (x) = h(x) T A U T i x h(x) T B V T i x<label>(13)</label></formula><p>Equation ( <ref type="formula" target="#formula_16">13</ref>) is more efficient in terms of parameters than equation (10) as it requires only 2(RdD + N R) parameters instead of 2N dD. We call this approach JCF for Joint Codebook and Factorization. This shared projection is both efficient in terms of number of parameters and in computation by a factor R / N . In the next section, we provide an ablation study of the proposed method, comparing equation ( <ref type="formula" target="#formula_12">10</ref>) and equation ( <ref type="formula" target="#formula_16">13</ref>), demonstrating that learning recombination is both efficient and performing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">IMPLEMENTATION DETAILS</head><p>In this section, we give some details about our implementation. All our experiments are performed on image retrieval datasets to assess the quality of the representations independantly of any classification scheme. We build our model over pre-trained network such as VGG16 <ref type="bibr" target="#b21">(Simonyan &amp; Zisserman (2014)</ref>) or ResNet50 ). In both case we reduce the features dimension to 256 dimensions and we l 2 -normalize them. For the assignment function h, we use the softmax over cosine similarity between the features and the codebook. Once the second-order representations are computed we pull them using global average pooling and we l 2 -normalize the output representation. Similarities between images are computed using the cosine similarity. In metric, we use Recall@K which takes the value 1 if there is at least one element from the same instance in the top-K results else 0 and averages these scores over the test set. The network is trained in 3 steps using a standard triplet loss function. In the first step, we freeze the ResNet50 and only train our added layers with 100 images per batch, we sample the negative within the batch and we use a learning rate of 10 −4 for 40 epochs. In the second one, we unfreeze ResNet50 and fine-tune the whole architecture for 40 epochs more with a learning rate of 10 −5 and a batch of 64 images with the negative sampled within the batch. In the last one, we fine-tune the network with a batch size of 64 images sampled by hard mining the training set with a learning rate of 10 −5 . The margin of the triplet loss is set to 0.1. Images are resized to 224x224 pixels for both train and test sets. In this section, we demonstrate both the relevance of second-order information for retrieval tasks and the influence of the codebook on our method. We report recall@1 on Stanford Online Products in Table <ref type="table">2</ref> for the different configuration detailed below with the training procedure from Section 3.4 without the hard mining step.</p><p>First, as a reference, we train a ResNet50 with a global average pooling and a fully connected layer to project the representation dimension from 2048 to 512. We denote it Baseline. Then we reimplement Bilinear Pooling (BP) and Compact Bilinear Pooling (CBP) and extend them naively to a codebook strategy (C-BP and C-CBP). The objective is to demonstrate that such strategy performs well, but a an intractable cost. Results are reported in Table <ref type="table">2</ref>. Note that, for each bilinear pooling method, we first add a 1 × 1 convolution to project the ResNet50 features from 2048 to 256 dimensions. This experiment confirm the interest of bilinear pooling in image retrieval with a improvement of 2% over the baseline, while using a 512 dimension representation. Furthermore, even using a codebook strategy with few codewords enhance bilinear pooling by 1% more, however, the number of parameters become intractable for codebook of size greater than 4: this naive strategy requires 270M parameters to extend this model to a codebook with a size of 8.</p><p>Using the factorization from equation ( <ref type="formula" target="#formula_12">10</ref>) greatly reduces the required number of parameters and allows the exploration of larger codebook. However, this factorization without codebook leads to lower scores than the non factorized bilinear pooling, but adding a codebook strategy increases performances by more than 4% over bilinear pooling without codebook, with nearly 4 times less parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SHARING PROJECTIONS</head><p>In this part, we study the impact of the sharing projection. We use the same training procedure as in the previous section. For each codebook size, we train architecture with a different number of projections, allowing to compare architectures without the sharing process to architectures with greater codebook size but with the same number of parameters by sharing projectors. Results are reported in Table <ref type="table">3</ref>. Sharing projectors leads to smaller models with few loss in performances, and using richer codebooks allows more compression with superior results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPARISON WITH BILINEAR FACTORIZATION</head><p>In this section, we report performances of our factorization on 3 fine-grained visual classification (FGVC) datasets: CUB <ref type="bibr">(Wah et al. (2011)</ref>), CARS <ref type="bibr" target="#b11">(Krause et al. (2013)</ref>) and AIRCRAFT <ref type="bibr" target="#b15">(Maji et al. (2013)</ref>). We use VGG16 as backbone network. Furthermore, to demonstrate the effectiveness of our codebook based factorization scheme to produce compact but effective second-order representations we compare JCF to closely-related formulations on FGVC tasks, that are:  </p><formula xml:id="formula_17">HPBP (z i (x) = p T i [σ(U T x) σ(V T x)]</formula><formula xml:id="formula_18">MR (z i (x) = x T U i V T i x).</formula><p>Multi-rank extension of Eq.( <ref type="formula" target="#formula_7">6</ref>) which allows to compare the benefit of codebook against direct rank increase. This approach is related to FBP which uses a higher rank decomposition (R = 20) than in our tests (R = 8).</p><formula xml:id="formula_19">MR+NL (z i (x) = 1 T R [σ(U T i x) σ(V T i x)]</formula><p>) Multi-rank with the same non-linearity as HPBP.</p><formula xml:id="formula_20">MR+NL+C (z i (x) = p T [σ(U T i x) σ(V T i x)]</formula><p>) which adds weights to the multi-rank combination. For a fair comparison, we fix the number of parameters for all of these methods to the same number as JCF (N = 32, R = 8). Thus, all methods use d = 256 and D = 512 and R = 8 except HPBP which uses R = 2048 to compensate for its shared matrices U , V .</p><p>We report classification accuracy on the three aforementioned datasets in Table <ref type="table" target="#tab_3">4</ref>. As we can see, our method consistently outperforms the multi-rank variants. This confirms our intuition about the importance of grouping features by similarity before projection and aggregation. Indeed, multirank variants do not have a selection mechanism preceding the projection into the subspace that would allow to selectively choose the projectors based on the input features. Instead, all features are projected using the same projectors and then aggregated. We argue that non-linear multi-rank variants bring only marginal improvements, since the non-linearity happens after the projection is made. Although it is still possible to learn a projection coupled with the non-linearity that would lead to a similarity driven aggregation, it is not enforced by design. Since JCF does the similarity driven aggregation by design, it is easier to train, which we believe explains the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">COMPARISON TO THE STATE-OF-THE-ART</head><p>In this section, we compare our method to the state-of-the-art on 3 retrieval datasets: Stanford Online Products (Oh <ref type="bibr" target="#b18">Song et al. (2016)</ref>), <ref type="bibr">CUB-200-2011</ref><ref type="bibr">(Wah et al. (2011</ref>) and <ref type="bibr">Cars-196 (Krause et al. (2013)</ref>). For Stanford Online Products and CUB-200-2011, we use the same train/test split as Oh <ref type="bibr" target="#b18">Song et al. (2016)</ref>. For Cars-196, we use the same as <ref type="bibr" target="#b19">Opitz et al. (2017)</ref>. We report the standard recall@K with K ∈ {1, 10, 100, 1000} for Stanford Online Products and with K ∈ {1, 2, 4, 8, 16, 32} for the other two.</p><p>On <ref type="bibr">CUB-200-2011 and Cars-196</ref> (see Table <ref type="table">6</ref>) we re-implement Bilinear Pooling (BP) and Compact Bilinear Pooling (CBP) on a VGG16. Even if the results are interesting, the constraint over intermediate representation is too strong to achieve relevant results. We then implement the codebook factorization from equation 10 a codebook size of 32 (denoted C-CBP). This formulation outperforms both classical second-order models by a large margin with few parameters. Moreover, our model that shares projections over the codebook (JCF , computed following equation 13) with R = 8 has 4 times less parameters for a 2% loss on <ref type="bibr">CUB-200-2011 dataset. On Cars-196</ref>, the sharing induces a higher loss, but may be improved with more projections to share.</p><p>On Stanford Online Products, we report the Baseline, implementations from equations 10 (C-CBP) and 13 (JCF ) with R = 8. We achieve state-of-the-art results using both methods and more than 10% improvement over the Baseline. Remark that JCF costs 4 times less than C-CBP at a 1% loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of second order methods. C. and F. columns are respectively for "Codebook" and "Factorization". Numbers in brackets are typical values. Methods marked † used the original paper values. Other methods use the following parameters: h = w = 28 are the height and width of the feature map, d = 512 is the feature dimension, D is the output representation, and is set to 512 if possible. Our proposed method uses a codebook N = 32 and a projection set of size R = 8.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Non-linear multi-rank with shared U , V .</figDesc><table><row><cell>Method</cell><cell cols="5">CUB CARS AIRCRAFT Feature dim. Parameters</cell></row><row><cell>Full BP -Lin et al. (2015)</cell><cell>84.1</cell><cell>90.6</cell><cell>86.9</cell><cell>256k</cell><cell>200MB</cell></row><row><cell>CBP-RM -Gao et al. (2016)</cell><cell>83.9</cell><cell>90.5</cell><cell>84.3</cell><cell>8192</cell><cell>38MB</cell></row><row><cell>CBP-TS -Gao et al. (2016)</cell><cell>84.0</cell><cell>91.2</cell><cell>84.1</cell><cell>8192</cell><cell>6.3MB</cell></row><row><cell>MoNet -Gou et al. (2018)</cell><cell>85.7</cell><cell>90.8</cell><cell>88.1</cell><cell>10k</cell><cell>4KB</cell></row><row><cell cols="2">LRBP -Kong &amp; Fowlkes (2017) 84.2</cell><cell>90.9</cell><cell>87.3</cell><cell>10k</cell><cell>0.8MB</cell></row><row><cell>FBP -Li et al. (2017a)</cell><cell>82.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SMSO -Yu &amp; Salzmann (2018)</cell><cell>85.0</cell><cell>-</cell><cell>-</cell><cell>2048</cell><cell>0.06MB</cell></row><row><cell>HPBP*</cell><cell>82.6</cell><cell>89.4</cell><cell>86.6</cell><cell>2048</cell><cell></cell></row><row><cell>MR MR+NL</cell><cell>83.1 82.3</cell><cell>89.0 89.4</cell><cell>85.9 85.5</cell><cell>512 512</cell><cell>8MB</cell></row><row><cell>MR+NL+C</cell><cell>82.4</cell><cell>89.5</cell><cell>86.5</cell><cell>512</cell><cell></cell></row><row><cell>JCF (N = 32, R = 8)</cell><cell>84.3</cell><cell>90.4</cell><cell>87.3</cell><cell>512</cell><cell>8MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of our proposed factorization scheme. We compare our method to the state-ofthe-art on Bilinear factorization and similar methods. We evaluate them with small representation dimension to attest our dimensionality reduction efficiency. * denotes our re-implementation.</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we propose a new pooling scheme based which is both efficient in performances (rich representation) and in representation dimension (compact representation). This is thanks to the second-order information that allows richer representation than first-order statistics and thanks to a codebook strategy which pools only related features. To control the computational cost, we extend this pooling scheme with a factorization that shares sets of projections between each entry of the codebook, trading fewer parameters and fewer computation for a small loss in performance. We achieve state-of-the-art results on Stanford Online Products and Cars-196, two image retrieval datasets. Even if our tests are performed on image retrieval datasets, we believe our method can readily be used in place of global average pooling for any task.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">All About VLAD</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2013.207</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN Architecture for Weakly Supervised Place Recognition</title>
		<author>
			<persName><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.572</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Clinchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w15-40</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality</title>
				<meeting>the Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting the VLAD image representation</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Delhumeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe-Henri</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2502081.2502171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Multimedia - MM &apos;13</title>
				<meeting>the 21st ACM international conference on Multimedia - MM &apos;13<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Constructing Distributed Representations Using Additive Clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/1120.003.0018</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2811" to="2819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compact Bilinear Pooling</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.41</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName><forename type="first">Ge</forename><surname>Weifeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MoNet: Moments Embedding Network</title>
		<author>
			<persName><forename type="first">Mengran</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavia</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Sznaier</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00335</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.90</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design and implementation of low-level machine learning API and API server</title>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Kyoung Seok Na</surname></persName>
		</author>
		<author>
			<persName><surname>Jae Min Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Jung Bin Park</surname></persName>
		</author>
		<author>
			<persName><surname>Jun Young Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Deok</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/icoin.2017.7899577</idno>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Information Networking (ICOIN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Low-Rank Bilinear Pooling for Fine-Grained Classification</title>
		<author>
			<persName><forename type="first">Shu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2017.743</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D Object Representations for Fine-Grained Categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccvw.2013.77</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision Workshops</title>
				<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factorized Bilinear Models for Image Recognition</title>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.229</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Scene Image Classification with the MFAFVNet</title>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.613</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear CNN Models for Fine-Grained Visual Recognition</title>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2015.170</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">NIPS (Nippan Information Processing System) and Nippan MARC System</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1241/johokanri.25.215</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Processing and Management</title>
		<title level="j" type="abbrev">joho kanri</title>
		<idno type="ISSN">0021-7298</idno>
		<idno type="ISSNe">1347-1597</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="230" />
			<date type="published" when="1982" />
			<publisher>Japan Science and Technology Agency (JST)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">No Fuss Distance Metric Learning Using Proxies</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.47</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Metric Learning via Lifted Structured Feature Embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.434</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BIER — Boosting Independent Embeddings Robustly</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.555</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving the Fisher Kernel for Large-Scale Image Classification</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-15561-1_11</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2010</title>
				<editor>
			<persName><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
			<persName><forename type="first">Petros</forename><surname>Maragos</surname></persName>
			<persName><forename type="first">Nikos</forename><surname>Paragios</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer Berlin Heidelberg</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><surname>Wei Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Yangqing Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep fishernet for object classification</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">07</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/11474.003.0014</idno>
	</analytic>
	<monogr>
		<title level="m">The Deep Learning Revolution</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4170" to="4178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiclass recognition and part localization with humans in the loop</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2011.6126539</idno>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Understanding the Public Health Implications of Prisoner Reentry in California: State-of-the-State Report</title>
		<author>
			<persName><surname>Dataset</surname></persName>
		</author>
		<idno type="DOI">10.1037/e525472012-001</idno>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>American Psychological Association (APA)</publisher>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grassmann Pooling as Compact Homogeneous Bilinear Pooling for Fine-Grained Visual Classification</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_22</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="365" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Krähenbühl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.10</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Statistically-Motivated Second-Order Pooling</title>
		<author>
			<persName><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
			<idno type="ORCID">0000-0002-0186-3399</idno>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
			<idno type="ORCID">0000-0002-8347-8637</idno>
		</author>
		<idno type="DOI">10.1007/978-3-030-01234-2_37</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision – ECCV 2018</title>
				<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="621" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hard-Aware Deeply Cascaded Embedding</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.94</idno>
		<idno>abs/1611.05720</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
