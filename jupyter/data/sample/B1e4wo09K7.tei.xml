<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.6.2" ident="GROBID" when="2022-03-08T15:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach to representation learning is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Representation learning <ref type="bibr" target="#b0">(Bengio et al., 2013)</ref> is part of the foundation of deep learning; powerful deep neural network models appear to derive their performance from sequentially representing data in more-and-more refined structures, tailored to the training task.</p><p>However, representation learning has a broader impact than just model performance. Transferable representations are leveraged efficiently for new tasks <ref type="bibr" target="#b22">(Mikolov et al., 2013)</ref>, representations are used for human interpretation of machine learning models <ref type="bibr" target="#b18">(Mahendran &amp; Vedaldi, 2015)</ref>, and meaningfully structured (disentangled) representations can be used for model control (e.g. semisupervised learning as in , topic modelling as in <ref type="bibr" target="#b1">Blei et al. (2003)</ref>).</p><p>Consequently, it is often preferable to have interpretable data representations within a model, in the sense that the information contained in the representation is easily understood and the representation can be used to control the output of the model (e.g. to generate data of a given class or with a particular characteristic). Unfortunately, there is often a tension between optimal model performance and cleanly disentangled or controllable representations.</p><p>To overcome this, some practitioners have proposed modifying their model's objective functions by inserting parameters in front of particular terms <ref type="bibr" target="#b2">(Bowman et al., 2016;</ref><ref type="bibr" target="#b11">Higgins et al., 2017)</ref>, while others have sought to modify the associated generative models <ref type="bibr" target="#b20">(Mansbridge et al., 2018)</ref>. Further still, attempts have been made to build the symmetries of the data directly into the neural network architecture in order to force the learning of latent variables that transform meaningfully under those symmetries <ref type="bibr" target="#b27">(Sabour et al., 2017)</ref>. The diversity and marginal success of these approaches point to the importance and difficulty of learning meaningful representations in deep generative modelling.</p><p>In this work we present an approach to probabilistic modelling of data comprised of a finite number of distinct classes, each described by a smooth manifold of instantiations of that class. For convenience, we call our approach EQUIVAE for Equivariant Variational Autoencoder. EQUIVAE is a probabilistic model with 2 latent variables: an invariant latent that represents the global class information, and an equivariant latent that smoothly interpolates between all of the members of that class. The EQUIVAE approach is general in that the symmetry group of the manifold must not be specified (as in for example <ref type="bibr" target="#b6">Cohen &amp; Welling (2014)</ref>; <ref type="bibr" target="#b8">Falorsi et al. (2018)</ref>), and it can be used for any number of classes and any dimensionality of both underlying representations. The price that must be paid for this level of model control and flexibility is that some labelled data is needed in order to provide the concept of class invariance versus equivariance to the model. The endeavor to model the content and the style of data separately is certainly not new to this work <ref type="bibr" target="#b29">(Tenenbaum &amp; Freeman, 2000)</ref>. <ref type="bibr" target="#b25">Reed et al. (2014)</ref> and <ref type="bibr" target="#b24">Radford et al. (2016)</ref> go further, disentangling the continuous sources of variation in their representations using a clamping technique that exposes specific latent components to a single source of variation in the data during training. In the same vein, other approaches have used penalty terms in the objective function that encourage the learning of disentangled representations <ref type="bibr" target="#b5">(Cheung et al., 2014;</ref><ref type="bibr" target="#b4">Chen et al., 2016)</ref>. EQUIVAE does not require any modification to the training algorithm, nor additional penalty terms in the objective function in order to bifurcate the information stored in the two latent variables. This is due to the way in which multiple data points are used to reconstruct a single data point from the same-class manifold, which we consider the primary novel aspect of our approach. In particular, our invariant representation takes as input multiple data points that come from the same class, but are different from the data point to be reconstructed. This invariant representation thus directly learns to encode the information common to the overall class, but not the individual data point, simply due to the information flowing through it.</p><p>Of further note, we deliberately use a deterministic latent for the invariant representation, and a stochastic latent for the smooth equivariant representation (an idea also employed by <ref type="bibr" target="#b30">Zhu et al. (2014)</ref>). This choice is why we do not need to explicitly force the equivariant latent to not contain any class-level information: it is available and easier to access from the deterministic latent.</p><p>EQUIVAE is also comparable to <ref type="bibr" target="#b28">Siddharth et al. (2017)</ref>, where the authors leverage labelled data explicitly in their generative model in order to force the VAE latent to learn the non-class information <ref type="bibr" target="#b19">(Makhzani et al. (2016)</ref> do similarly using adversarial training). The primary difference between those works and ours is that EQUIVAE provides a non-trivial representation of the global information instead of simply using the integer-valued label. Furthermore, this invariant representation can be deterministically evaluated directly on unlabelled data. Practitioners can reuse this embedding on unlabelled data in downstream tasks, along with the equivariant encoder if needed. The invariant representation provides more information than a simple prediction of the class-label distribution.</p><p>The encoding procedure for the invariant representation in EQUIVAE is partially inspired by <ref type="bibr" target="#b7">Eslami et al. (2018)</ref>, who use images from various, known coordinates in a scene in order to reconstruct a new image of that scene at new, known coordinates. In contrast, we do not have access to the exact coordinates of the class instance, which in our case corresponds to the unknown, non-trivial manifold structure of the class; we must infer these manifold coordinates in an unsupervised way. <ref type="bibr" target="#b9">Garnelo et al. (2018a;</ref> similarly explore the simultaneous usage of multiple data points in generative modelling in order to better capture modelling uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">EQUIVARIANT VARIATIONAL AUTOENCODERS</head><p>We consider a generative model for data comprised of a finite set of distinct classes, each of which occupies a smooth manifold of instantiations. For example, images of distinct objects where each object might be in any pose, or sentences describing distinct sets of topics. Such data should be described by a generative model with two latent variables, the first describing which of the objects the data belongs to, and the second describing the particular instantiation of the object (e.g. its pose).</p><p>In this way the object-identity latent variable r would be invariant under the transformations that cover the set of possible instantiations of the object, and the instantiation-specifc latent variable v should be equivariant under such transformations. Note that the class label y is itself an invariant representation of the class, however, we seek a higher-dimensional latent vector r that has the capacity to represent rich information relevant to the class of the data point, rather than just its label.</p><p>Denoting an individual data point as x n with associated class label y n , and the full set of class-y labeled data {x n |label(x n ) = y} as D y lab , we write such a generative model as:</p><formula xml:id="formula_0">p {x n , y n } N n=1 = N n=1 dv n dr n p θ (x n |r n , v n ) δ r n − r(D yn lab \ {x n }) p(v n ) p(y n ) (1)</formula><p>where θ are the parameters of the generative model. We make explicit with a δ function the conditional dependency of p θ on the deterministically calculable representation r n of the global properties of class y n . The distribution p(y n ) is a categorical distribution with weights given by the relative frequency of each class and the prior distribution p(v n ) is taken to be a unit normal describing the set of smooth transformations that cover the class-y n manifold.</p><p>To guarantee that r n will learn an invariant representation of information common to the class-y n data, we use a technique inspired by Generative Query Networks <ref type="bibr" target="#b7">(Eslami et al., 2018)</ref>. Instead of encoding the information of a single data point (x n , y n ) into r n , we provide samples from the whole class-y n manifold. That is, we compute the invariant latent as:</p><formula xml:id="formula_1">r D yn lab \ {x n } concise notation = r yn = E x∼D yn lab \{xn} f θinv (x) ≈ 1 m m i=1 f θinv (x i ) (2)</formula><p>where θ inv are the parameters of this embedding. We explicitly exclude the data point at hand x n from this expectation value; in the infinite labelled data limit, the probability of sampling x n from D yn lab would vanish. We include the simplified notation r yn for subsequent mathematical clarity. This procedure invalidates the assumption that the data is generated i.i.d. conditioned on a set of model parameters, since r y is computed using a number of other data points x i with label y. For notational simplicity, we will suppress this fact, and consider the likelihood p(x, y) as if it were i.i.d. per data point. It is not difficult to augment the equations that follow to incorporate the full dependencies, but we find this to obfuscate the discussion (see Appendix B for full derivations). We ignore the bias introduced from the non-i.i.d. generation process; this could be avoided by holding out a dedicated labelled data set <ref type="bibr" target="#b9">(Garnelo et al., 2018a;</ref>, but we find it empirically insignificant.</p><p>The primary purpose of our approach to the invariant representation used in Equation 2 is to provide exactly the information needed to learn a global-class embedding: namely, to learn what the elements of the class manifold have in common. However, our approach provides a secondary advantage. During training we will use values of m (see Equation <ref type="formula">2</ref>) sampled uniformly between 1 and some small maximal value m max . The r y embedding will thus learn to work well for various values of m, including m = 1. Consequently, at inference time, any unlabelled data point x can be immediately embedded via f θinv (x). This is ideal for downstream usage; we will use this technique in Section 3.1 to competitively classify unlabelled test-set data using only f θinv (x).</p><p>In order to approximate the integral in Equation 1 over the equivariant latent, we use variational inference following the standard VAE approach :</p><formula xml:id="formula_2">q φcov (v|r y , x) = N µ φcov (r y , x), σ 2 φcov (r y , x)I (3)</formula><p>where φ cov are the parameters of the variational distribution over the equivariant latent. Note that v is inferred from r y (and x), not y, since r y is posited to be a multi-dimensional latent vector that represents the rich set of global properties of the class y, rather than just its label. As is shown empirically in Section 3, v only learns to store the intra-class variations, rather than the class-label information. This is because the class-label information is easier to access directly from the deterministic representation r y , rather than indirectly through the stochastic v. We choose to provide both r y and x to q φcov (v|r y , x) in order to provide the variational distribution with more flexibility.</p><p>We thus arrive at a lower bound on log p(x, y) given in Equation <ref type="formula">1</ref>following the standard arguments:</p><formula xml:id="formula_3">L lab = E q(v|ry,x) log p(x|r y , v) − D KL q(v|r y , x) p(v) + log p(y)<label>(4)</label></formula><p>where the various model parameters are suppressed for clarity.</p><p>The intuition associated with the inference of r y from multiple same-class, but complementary data points, as well as the inference of v from r y and x is depicted heuristically in Figure <ref type="figure">1</ref>.</p><p>EQUIVAE is designed to learn a representation that stores global-class information, and as such, it can be used for semi-supervised learning. Thus, an objective function for unlabelled data must be specified to accompany the labelled-data objective function given in Equation <ref type="formula" target="#formula_3">4</ref>.</p><p>We marginalise over the label in p(x, y) (Equation <ref type="formula">1</ref>) when a data point is unlabelled. In order to perform variational inference in this case, we use a variational distribution of the form:</p><formula xml:id="formula_4">q(v, y|x) = q φcov (v|r y , x) q φy-post (y|x)<label>(5)</label></formula><p>Figure <ref type="figure">1</ref>: Heuristic depiction of class-y manifold. The invariant latent r y will encode globalmanifold information, whereas equivariant latent v will encode coordinates of x on the manifold.</p><p>where q φcov (v|r y , x) is the same distribution as is used in the labelled case, given in Equation <ref type="formula">3</ref>. The unlabelled setting requires an additional inference distribution to infer the label y, which is achieved with q φy-post (y|x), parametrised by φ y-post . Once y is inferred from q φy-post (y|x), r y can be deterministically calculated using Equation 2 from the labelled data set D y lab for class y, of which x is no longer a part. With r y and x, the equivariant latent v is inferred via q φcov (v|r y , x).</p><p>Using this variational inference procedure, we arrive at a lower bound for log p(x):</p><formula xml:id="formula_5">L unlab = E q(y|x) E q(v|ry,x) log p(x|r y , v) − D KL q(v|r y , x) p(v) − D KL q(y|x) p(y) (6)</formula><p>where the model parameters are again suppressed for clarity.</p><p>We will compute the expectation over the discrete distribution q(y|x) in Equation <ref type="formula">6</ref>exactly in order to avoid the problem of back propagating through discrete variables. However, this expectation could be calculated by sampling using standard techniques <ref type="bibr" target="#b3">(Brooks et al., 2011;</ref><ref type="bibr" target="#b12">Jang et al., 2017;</ref><ref type="bibr" target="#b17">Maddison et al., 2017)</ref>.</p><p>Therefore, the evidence lower bound objective for semi-supervised learning becomes:</p><formula xml:id="formula_6">x, unlab. log p(x) + (x,y), lab. log p(x, y) ≥ L semi = x, unlab. L unlab + (x,y), lab. L lab (7)</formula><p>In order to ensure that q φy-post (y|x) does not collapse into the local minimum of predicting a single label for every x value, we add log q φy-post (y|x) to L lab . This is done in  and <ref type="bibr" target="#b28">Siddharth et al. (2017)</ref>, however, we do not add any hyperparameter in front of this term unlike those works. We also do not add a hyperparameter up-weighting L lab overall, as is done in <ref type="bibr" target="#b28">Siddharth et al. (2017)</ref>. The only hyperparameter tuning we perform is to choose latent dimensionality (either 8 or 16) and to choose m max , between 1 and which m (see Equation <ref type="formula">2</ref>) varies uniformly during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RESULTS</head><p>We carry out experiments on both the MNIST data set <ref type="bibr" target="#b16">(LeCun et al., 1998)</ref> and the Street View House Numbers (SVHN) data set <ref type="bibr" target="#b23">(Netzer et al., 2011)</ref>. These data sets are appropriately modelled with EQUIVAE, since digits from a particular class live on a smooth, a-priori-unknown manifold.</p><p>EQUIVAE requires some labelled data. Forcing the model to reconstruct x through a representation r y that only has access to other members of the y class is what forces r y to represent the common information of that class, rather than a representation of the particular instantiation x. Thus, the requirement of some labelled data is at the heart of EQUIVAE. Indeed, we trained several versions of the EQUIVAE generative model in the unsupervised setting, allowing r to receive x directly. The results were as expected: the equivariant latent is completely unused, with the model unable to reconstruct the structure in each class, as it essentially becomes a deterministic autoencoder.</p><p>In Section 3.1, we study the invariant-equivariant properties of the representations learnt with EQUIVAE in the supervised setting, where we have access to the full set of labelled training data. The semi-supervised learning setting is discussed in Section 3.2. The details of the experimental setup used in this section are provided in Appendix A. The training curves on MNIST are shown on the right in Figure <ref type="figure">2</ref>. The equivariant latent is learning to represent non-trivial information from the data as evidenced by the KL between the equivariant variational distribution and its prior not vanishing at convergence.</p><p>However, when visualised in 2 dimensions using UMAP for dimensional reduction <ref type="bibr" target="#b21">(McInnes &amp; Healy, 2018)</ref>, the equivariant latent v appears not to distinguish between digit classes, as is seen in the uniformity of the class-coloured labels in the middle plot of Figure <ref type="figure">2</ref>. The apparent uniformity of v reflects two facts: the first is that, given that the generative model gets access to another latent containing the global class information, the equivariant latent does not need to distinguish between classes. The second is that the equivariant manifolds should be similar across all MNIST digits. Indeed, they all include rotations, stretches, stroke thickness, among smooth transformations.</p><p>Finally, on the left in Figure <ref type="figure">2</ref>, the invariant representation vectors r y are shown, dimensionally reduced to 2 dimensions using UMAP, and coloured according to the label of the class. These representations are well separated for each class. Each class has some spread in its invariant representations due to the fact that we choose relatively small numbers of complementary samples, m (see Equation <ref type="formula">2</ref>). The model shown in Figure <ref type="figure">2</ref> had m randomly selected between 1 and 7 during training, with m = 5 used for visualisation. The outlier points in this plot are exaggerated by the dimensional reduction; we show this below in Figure <ref type="figure" target="#fig_0">4</ref> by considering a EQUIVAE with 2D latent.</p><p>The SVHN results are similar to Figure <ref type="figure">2</ref>, with slightly less uniformity in the equivariant latent.</p><p>All visualisations in this work, including those in Figure <ref type="figure">2</ref>, use data from the validation set (5,000 images for MNIST; 3,257 for SVHN). We reserve the test set (10,000 images for MNIST; 26,032 for SVHN) for computing the accuracy values provided. We did not look at this test set during training and hyperparameter tuning. In terms of hyperparameter tuning, we only tuned the number of epochs for training, the range of m values (i.e. m max = 7 for MNIST; m max = 10 for SVHN), and chose between 8 and 16 for the dimensionality of both latents (16 chosen for both data sets). We fixed the architecture at the outset to have ample capacity for this task, but did not vary it in our experiments.</p><p>In order to really see what information is stored in the equivariant and invariant latents, we consider them in the context of the generative model. We show reconstructed images in various latent-variable configurations in Figure <ref type="figure">3</ref>. To show samples from the equivariant prior p(v) we fix a single invariant representation r y for each class y by taking the mean r y over each class in the validation set. On the left in Figure <ref type="figure">3</ref> we show random samples from p(v) reconstructed along with r y for each class y ascending from 0 to 9 in each column. Two properties stand out from these samples: firstly, the Figure <ref type="figure">3</ref>: Generated images (left) sampled from the prior p(v) for each r y , (middle) reconstructed from equivariant interpolations between the embeddings of same-class digits with fixed r y , and (right) reconstructed from latent pairs (r i y , v j ) where (r i y , v i ) is an encoded image with y = i.</p><p>samples are (almost) all from the correct class for MNIST (SVHN), showing that the invariant latent is representing the class information well. Secondly, there is appreciable variance within each class, showing that samples from the prior p(v) are able to represent the intra-class variations.</p><p>The middle plot in Figure <ref type="figure">3</ref> shows interpolations between actual digits of the same class (the top and bottom rows of each subfigure), with the invariant representation fixed throughout. These interpolations are smooth, as is expected from interpolations of a VAE latent, and cover the trajectory between the two images well. This again supports the argument that the equivariant representation v, as a stochastic latent variable, is appropriate for representing the smooth intra-class transformations.</p><p>To create the right-hand side of Figure <ref type="figure">3</ref>, a validation-set image for each digit i is encoded to create the set of latents {r i y=i , v i } 9 i=0 , from which we reconstruct images using the latent pairs (r i y=i , v j ) for i, j = 0, . . . , 9. Thus we see in each row a single digit and in each column a single style. It is most apparent in the SVHN results that the equivariant latent controls all stylistic aspects of the image, including the non-central digits, whereas the invariant latent controls only the central digit.</p><p>In Figure <ref type="figure" target="#fig_0">4</ref> we show an EQUIVAE trained with 2-dimensional invariant and equivariant latents on MNIST for clearer visualisation of the latent space. On the right, reconstructions are shown with fixed r y for each y = 3, 4, 5, 6 and with the identical set of evenly spaced v over a grid spanning from −2 to +2 in each coordinate (2 prior standard deviations). The stylistic variations appear to be similar for the same values of v across different digits, as was partially evidence on the right of Figure <ref type="figure">3</ref>. On the left of Figure <ref type="figure" target="#fig_0">4</ref> we see where images in the validation set are encoded, showing significant distance between clusters when dimensional reduction is not used. Finally, in the middle of Figure <ref type="figure" target="#fig_0">4</ref> we see the evenly-spaced reconstruction of the full invariant latent space. This shows that, though the invariant latent is not storing stylistic information, it does contain the relative similarity of the base version of each digit. This lends justification to our assertion that the invariant latent r y represents more information that just the label y.  12.30 ± 0.28</p><p>We have thus seen that the invariant representation r y in EQUIVAE learns to represent globalclass information and the equivariant representation v learns to represent local, smooth, intra-class information. This is what we expected from the theoretical considerations given in Section 2.</p><p>We now show quantitatively that the invariant representation r y learns the class information by showing that it alone can predict the class y as well as a dedicated classifier. In order to predict an unknown label, we employ a direct technique to compute the invariant representation r y from x. We simply use f θinv (x) from Equation <ref type="formula">2</ref>. We can then pass f θinv (x) into a neural classifier, or we can find the nearest cluster mean r y from the training set and assign class probabilities according to p(label(x) = y) ∝ exp(−||f θinv (x) − r y || 2 ). We find that classifying test-set images using this 0-parameter distance metric performs as well as using a neural classifier (2-layer dense dropout network with 128, 64 neurons per layer) with f θinv (x) as input. Note that using p(y|x) ∝ p(x, y) is roughly equivalent to our distance-based classifier, as p(y|x) will be maximal when r y (computed from the training data with label y) is most similar to f θinv (x). Thus, our distance-based technique is a more-direct approach to classification than using p(y|x).</p><p>Our results are shown in Table <ref type="table" target="#tab_1">1</ref>, along with the error rate of a dedicated, end-to-end neural network classifier, identical in architecture to that of f θinv (x), with 2 dropout layers added. This benchmark classifier performs similarly on MNIST (slightly better on SVHN) to the simple classifier based on finding the nearest training-set cluster to f θinv (x). This is a strong result, as our classification algorithm based on f θinv (x) has no direct classification objective in its training, see Equation <ref type="formula" target="#formula_3">4</ref>. Our uncertainty bands quoted in Table <ref type="table" target="#tab_1">1</ref> use the standard error on the mean (standard deviation divided by √ N − 1), with N = 5 trials.</p><p>Results from selected, relevant works from the literature are also shown in Table <ref type="table" target="#tab_1">1</ref>.  do not provide error bars, and they only provide a fully supervised result for their most-  <ref type="bibr" target="#b19">Makhzani et al. (2016)</ref> perform slightly worse than EQUIVAE, but within error bars. <ref type="bibr" target="#b19">Makhzani et al. (2016)</ref> train for 100 times more epochs than we do, and use over 10 times more parameters in their model, although they use shallower dense networks. Note also that  and <ref type="bibr" target="#b19">Makhzani et al. (2016)</ref> train on a training set of 50,000 MNIST images, whereas we use 55,000. We are unable to compare to <ref type="bibr" target="#b28">Siddharth et al. (2017)</ref> as they do not provide fully supervised results on MNIST, and none of these comparable approaches provide fully supervised results on SVHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SEMI-SUPERVISED LEARNING</head><p>For semi-supervised learning, we maximise L semi given in Equation <ref type="formula">7</ref>. Test-set classification error rates are presented in Table <ref type="table" target="#tab_2">2</ref> for varying numbers of labelled data. We compare to a benchmark classifier with similar architecture to q φy-post (y|x) (see Equation <ref type="formula" target="#formula_4">5</ref>) trained only on the labelled data, as well as to similar VAE-based semi-supervised work. The number of training epochs are chosen to be 20, 25, 30, and 35, for data set sizes 100, 600, 1000, and 3000, respectively, on MNIST, and 20 and 30 epochs for data set sizes 1000, and 3000, respectively, on SVHN. We use an 8D latent space and m max = 4 for MNIST, and an 16D latent space and m max = 10 for SVHN. Otherwise, no hyperparameter tuning is performed. Each of our experiments is run 5 times to get the mean and (standard) error on the estimate of the mean in Table <ref type="table" target="#tab_2">2</ref>.</p><p>We find that EQUIVAE performs better than the benchmark classifier with the same architecture (plus two dropout layers appended) trained only on the labelled data, especially for small labelled data sets. Furthermore, EQUIVAE performs competitively (within error bars or better) relative to its most similar comparison, <ref type="bibr" target="#b28">Siddharth et al. (2017)</ref>, which is a VAE-based probabilistic model that treats the labels and the style of the data separately. Given its relative simplicity, rapid convergence (20-35 epochs), and lack of hyperparameter tuning performed, we consider this to be an indication that EQUIVAE is an effective approach to jointly learning invariant and equivariant representations, including in the regime of limited labelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSIONS</head><p>We have introduced a technique for jointly learning invariant and equivariant representations of data comprised of discrete classes of continuous values. The invariant representation encodes global information about the given class manifold which is ensured by the procedure of reconstructing a data point through complementary samples from the same class. The equivariant representation is a stochastic VAE latent that learns the smooth set of transformations that cover the instances of data on that class manifold. We showed that the invariant latents are so widely separated that a 99.18% accuracy can be achieved on MNIST (87.70% on SVHN) with a simple 0-parameter distance metric based on the invariant embedding. The equivariant latent learns to cover the manifold for each class of data with qualitatively excellent samples and interpolations for each class. Finally, we showed that semi-supervised learning based on such latent variable models is competitive with similar approaches in the literature with essentially no hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXPERIMENTAL SETUP</head><p>In this appendix we provide details of the experimental setup that was used to generate the results from Section 3.</p><p>For our implementation of EQUIVAE, we use relatively standard neural networks. All of our experiments use implementations with well under 1 million parameters in total, converge within a few hours (on a Tesla K80 GPU), and are exposed to minimal hyperparameter tuning.</p><p>In particular, for the deterministic class-representation vector r y given in Equation <ref type="formula">2</ref>, we parametrise f θinv (x) using a 5-layer, stride-2 (stride-1 first layer), with 5x5 kernal size, convolution network, followed by a dense hidden layer. The mean of these m embeddings f θinv (x i y ) is taken, followed then by another dense hidden layer, and the final linear dense output layer. This is shown for a y = 6 MNIST digit in the top shaded box of Figure <ref type="figure" target="#fig_1">5</ref>. <ref type="bibr">Our implementation uses (8,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">64)</ref> filters in the convolution layers, and (128, 64) hidden units in the two subsequent dense layers for a 16 dimensional latent (the number of units in the dense layers are halved when using 8 dimensional latents, as in our semi-supervised experiments on MNIST).</p><p>We parametrise the approximate posterior distribution q φcov (v|r y , x) over the equivariant latent as a diagonal-covariance normal distribution, N (µ φcov (r y , x), σ 2 φcov (r y , x)), following the SGVB algorithm . For µ φcov (r y , x) and σ 2 φcov (r y , x), we use the identical convolution architecture as for the invariant embedding network as an initial embedding for the data point x. This embedding is then concatenated with the output of a single dense layer that transforms r y , the output of which is then passed to one more dense hidden layer for each µ and σ 2 separately. This is shown in the bottom shaded box of Figure <ref type="figure" target="#fig_1">5</ref>.</p><p>The generative model p θ (x|r y , v) is based on the DCGAN-style transposed convolutions <ref type="bibr" target="#b24">(Radford et al., 2016)</ref>, and is assumed to be a Bernoulli distribution for MNIST (Gaussian distribution for SVHN) over the conditionally independent image pixels. Both the invariant representation r y and the equivariant representation v, are separately passed through a single-layer dense network before being concatenated and passed through another dense layer. This flat embedding that combines both representations is then transpose convolved to get the output image in a way the mirrors the 5-layer convolution network used to embed the representations in the first place. That is, we use (64, 128) hidden units in the first two dense layers, and then (64, 32, 16, 8, n colours ) filters in each transpose convolution layer, all with 5x5 kernals and stride 2, except the last layer, which is a stride-1 convolution layer (with padding to accommodate different image sizes).</p><p>In our semi-supervised experiments, we implement q φy-post (y|x) using the same (5-CNN, 1dense) encoding block to provide an initial embedding for x. This is then concatenated with stop grad(f θinv (x)) and passed to a 2-layer dense dropout network with (128, 64) units. The use of stop grad(f θinv (x)) is simply that f θinv (x) is learning a highly relevant, invariant representation of x that q φy-post (y|x) might as well get access to. However, we do not allow gradients to pass through this operation since f θinv (x) is meant to learn from the complementary data of known same-class members only.</p><p>As discussed in Section 2, the number of complementary samples m used to reconstruct r y (see Equation <ref type="formula">2</ref>) is chosen randomly at each training step in order to ensure that r y is insensitive to m. For our supervised experiments where labelled data are plentiful, m is randomly select between 1 and m max with m max = 7 for MNIST (m max = 10 for SVHN), whereas in the semi-supervised case m max = 4 for MNIST (m max = 10 for SVHN).</p><p>We perform standard, mild preprocessing on our data sets. MNIST is normalised so that each pixel value lies between 0 and 1. SVHN is normalised so that each pixel has zero mean and unit standard deviation over the entire dataset.</p><p>Finally, all activation functions that are not fixed by model outputs are taken to be rectified linear units. We use Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba, 2015)</ref> for training with default settings, and choose a batch size of 32 at the beginning of training, which we double successively throughout training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DERIVATION OF LIKELIHOOD LOWER BOUNDS</head><p>In this appendix we detail the derivations of the log-likelihood lower bounds that were provided in Section 2.</p><p>EQUIVAE is relevant when a non-empty set of labelled data is available. We write the data set as</p><formula xml:id="formula_7">D = D lab ∪ D unlab = {x n , y n } Nlab n=1 ∪ {x n } Nunlab n=1<label>(8)</label></formula><p>We also decompose D lab = ∪ y D y lab , where D y lab is the set of labelled instantiations x with label y. In particular, in what follows we think of D y lab as containing only the images x, not the labels, since they are specified by the index on the set. We require at least two labelled data points from each class, so that |D y lab | ≥ 2 ∀y. We would like to maximise the log likelihood that our model generates both the labelled and the unlabelled data, which we write as: log p(D) = log p(D lab ) + log p(D unlab |D lab )</p><p>where we make explicit here the usage of labelled data in the unlabelled generative model.</p><p>For convenience, we begin by repeating the generative model for the labelled data in Equation <ref type="formula">1</ref>, except with the deterministic integral over r n completed:</p><formula xml:id="formula_9">p {x n , y n } Nlab n=1 = Nlab n=1 dv n p θ x n |r(D yn lab \ {x n }), v n p(v n ) p(y n )<label>(10)</label></formula><p>We will simplify the notation by writing x n = D yn lab \ {x n } and r yn, xn = r(D yn lab \ {x n }), but keep all other details explicit.</p><p>We seek to construct a lower bound on log p({x n , y n } Nlab n=1 ), namely the log likelihood of the labelled data, using the following variational distribution over v n (Equation <ref type="formula">3</ref> Which coincides with the notationally simplified lower bound objective function given in Equation <ref type="formula" target="#formula_3">4</ref>.</p><p>We now turn to the lower bound on the unlabelled data. To start, we marginalise over the labels on the unlabelled dataset:</p><formula xml:id="formula_10">p {x n } Nunlab n=1 D lab = Nunlab n=1 yn dv n p θ x n |r(D yn lab ), v n p(v n ) p(y n )<label>(13)</label></formula><p>where we no longer need to remove x n from D yn lab in r(•) since for the unlabelled data, x n ∈ D yn lab . As was done for the labelled data, we construct a lower bound using variational inference. However, in this case, we require a variational distribution over both y n and v n . We take: Thus, we have Equation <ref type="formula">6</ref>augmented with the notational decorations that were omitted in Section 2.</p><p>Therefore, the objective</p><formula xml:id="formula_11">L = Nunlab n=1 L (n) unlab + Nlab n=1 L (n) lab (<label>16</label></formula><formula xml:id="formula_12">)</formula><p>given in Equation <ref type="formula">7</ref>, with L</p><p>(n) lab given in Equation <ref type="formula" target="#formula_3">4</ref>and L</p><p>(n)</p><p>unlab given in Equation <ref type="formula">6</ref>, is a lower bound on the data log likelihood.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Latent variables for EQUIVAE with 2 dimensional latents. The invariant latent space (left), reconstructions from evenly-spaced variations of r y covering the full space at fixed v = 0 (middle), and reconstructions from 2-prior-standard-deviation variations of v at fixed r y (right) are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of variational encoding of a particular MNIST digit x (from the 6 class) both in terms of its invariant representation (top) and its equivariant representation (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>EE</head><label></label><figDesc>):q φcov (v n |r yn, xn , x n ) = N µ φcov (r yn, xn , x n ), σ 2 φcov (r yn, xn , x n )I (11) Indeed, log p {x n , y n } Nlab n=1 = Nlab n=1 log E q φcov (vn|r yn , xn ,xn) p θ x n |r yn, xn , v n p(v n ) p(y n ) q φcov (v n |r yn, xn , x n ) q φcov (vn|r yn , xn ,xn) log p θ x n |r yn, xn , v n p(v n ) p(y n ) q φcov (v n |r yn, xn , x n ) q φcov (vn|r yn , xn ,xn) log p θ x n |r yn, xn , v n − D KL q φcov (v n |r yn, xn , x n ) p(v n ) + log p(y n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>q(v n , y n |x n , D lab ) = q φcov v n r(D yn lab ), x n q φy-post (y n |x n ) (14) which gives log p {x n } Nunlab n=1 D lab = log Nunlab n=1 E q φ y-post (yn|xn) E q φcov (vn|r(D yn lab ),xn) p θ x n |r(D yn lab ), v n p(v n ) p(y n ) q φcov v n r(D yn lab ), x n q φy-post (y n |x n ) q φ y-post (yn|xn) E q φcov (vn|r(D yn lab ),xn) log p θ x n |r(D yn lab ), v n p(v n ) p(y n ) q φcov v n r(D yn lab ), x n q φy-post (y n |x n ) = Nunlab n=1 E q φ y-post (yn|xn) E q φcov (vn|r(D yn lab ),xn) log p θ x n |r(D yn lab ), v n (15) − D KL q φcov (v n |r(D yn lab ), x n ) p(v n ) − D KL q φy-post (y n |x n ) p(y n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 2: Validation-set results for EQUIVAE on MNIST. Invariant (left) and equivariant (middle) latent representations are shown reduced to 2D using UMAP. Learning curves are shown (right) with the ELBO broken down into the sum of the reconstruction and (negative) KL terms, as in Equation4.</figDesc><table /><note>3.1 SUPERVISED LEARNINGWith the full training data set labelled, EQUIVAE is able to learn to optimise both equivariant and invariant representations at every training step. The supervised EQUIVAE (objective given in Equation4) converges in approximately 40 epochs on the MNIST training datset of 55,000 data points and in 90 epochs on the SVHN training datset of 70,000 data points.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Supervised error rates on MNIST (10,000 images) and SVHN (26,032 images) test sets.</figDesc><table><row><cell></cell><cell>Technique</cell><cell>Error rate</cell></row><row><cell></cell><cell>EQUIVAE Benchmark neural classifier</cell><cell>0.84 ± 0.03</cell></row><row><cell>MNIST</cell><cell>EQUIVAE (neural classifier using f θinv (x)) EQUIVAE (distance based on f θinv (x)) Stacked VAE (M1+M2) (Kingma et al., 2014)</cell><cell>0.82 ± 0.03 0.82 ± 0.05 0.96</cell></row><row><cell></cell><cell>Adversarial Autoencoders (Makhzani et al., 2016)</cell><cell>0.85 ± 0.02</cell></row><row><cell>SVHN</cell><cell>EQUIVAE Benchmark neural classifier EQUIVAE (neural classifier using f θinv (x)) EQUIVAE (distance based on f θinv (x))</cell><cell>10.04 ± 0.14 11.97 ± 0.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semi-supervised test-set error rates for various labelled-data-set sizes.</figDesc><table><row><cell></cell><cell cols="2">Labels EQUIVAE</cell><cell cols="3">Benchmark Siddharth et al. (2017) Kingma et al. (2014)</cell></row><row><cell>MNIST</cell><cell>100 600 1000</cell><cell cols="2">8.90 ± 0.70 21.91 ± 0.66 3.99 ± 0.17 6.64 ± 0.35 3.34 ± 0.17 5.43 ± 0.31</cell><cell>9.71 ± 0.91 3.84 ± 0.86 2.88 ± 0.79</cell><cell>(M2) 11.97 ± 1.71 4.94 ± 0.13 3.60 ± 0.56</cell></row><row><cell></cell><cell>3000</cell><cell>2.23 ± 0.14</cell><cell>2.96 ± 0.11</cell><cell>1.57 ± 0.93</cell><cell>3.92 ± 0.63</cell></row><row><cell>SVHN</cell><cell cols="3">1000 37.95 ± 0.66 39.64 ± 1.47 3000 24.95 ± 0.57 25.50 ± 0.91</cell><cell>38.91 ± 1.06 29.07 ± 0.83</cell><cell>(M1+M2) 36.02 ± 0.10 -</cell></row><row><cell cols="6">powerful, stacked / pre-trained VAE M1+M2, but it appears as if their learnt representation is less</cell></row><row><cell cols="4">accurate in its classification of unlabelled data.</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="DOI">10.1109/tpami.2013.50</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<title level="j" type="abbrev">IEEE Trans. Pattern Anal. Mach. Intell.</title>
		<idno type="ISSN">0162-8828</idno>
		<idno type="ISSNe">2160-9292</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013-08" />
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational methods for the Dirichlet process</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="DOI">10.1145/1015330.1015439</idno>
	</analytic>
	<monogr>
		<title level="m">Twenty-first international conference on Machine learning - ICML &apos;04</title>
				<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xiao-Li</surname></persName>
		</author>
		<idno type="DOI">10.1201/b10905</idno>
		<imprint>
			<date type="published" when="2011-05-10" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discovering hidden factors of variation in deep networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Livezey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6583</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Irreducible tensor representations and Young tableau</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1142/9789814603287_0005</idno>
	</analytic>
	<monogr>
		<title level="m">Lie Groups and Lie Algebras for Physicists</title>
				<imprint>
			<publisher>WORLD SCIENTIFIC</publisher>
			<date type="published" when="2014-09-25" />
			<biblScope unit="page" from="125" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural scene representation and rendering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M Ali</forename><surname>Eslami</surname></persName>
			<idno type="ORCID">0000-0003-1838-5589</idno>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
			<idno type="ORCID">0000-0003-3184-8509</idno>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Besse</surname></persName>
			<idno type="ORCID">0000-0002-0854-3857</idno>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
			<idno type="ORCID">0000-0002-6810-7838</idno>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
			<idno type="ORCID">0000-0001-5370-6515</idno>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avraham</forename><surname>Ruderman</surname></persName>
			<idno type="ORCID">0000-0002-6044-5218</idno>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
			<idno type="ORCID">0000-0001-9395-8223</idno>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theophane</forename><surname>Weber</surname></persName>
			<idno type="ORCID">0000-0001-6230-952X</idno>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
			<idno type="ORCID">0000-0001-7758-6896</idno>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aar6170</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<title level="j" type="abbrev">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<idno type="ISSNe">1095-9203</idno>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6394</biblScope>
			<biblScope unit="page" from="1204" to="1210" />
			<date type="published" when="2018-06-15" />
			<publisher>American Association for the Advancement of Science (AAAS)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>De Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Forré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<idno>1807.04689</idno>
		<title level="m">Explorations in homeomorphic variational auto-encoding</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<title level="m">Conditional neural processes. In International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<title level="m">Neural processes</title>
				<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.003.0203</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<title level="j" type="abbrev">Proc. IEEE</title>
		<idno type="ISSN">0018-9219</idno>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
			<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The concrete distribution: a continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2015.7299155</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial autoencoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving latent variable descriptiveness by modelling rather than ad-hoc factors</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Mansbridge</surname></persName>
			<idno type="ORCID">0000-0001-7557-9668</idno>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Fierimonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Feige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-019-05830-1</idno>
		<idno>1806.04480</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<title level="j" type="abbrev">Mach Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<idno type="ISSNe">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">8-9</biblScope>
			<biblScope unit="page" from="1601" to="1611" />
			<date type="published" when="2019-07-22" />
			<publisher>Springer Science and Business Media LLC</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">UMAP: Uniform Manifold Approximation and Projection</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
			<idno type="ORCID">0000-0003-2143-6834</idno>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Großberger</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.00861</idno>
		<idno>1802.03426</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<title level="j" type="abbrev">JOSS</title>
		<idno type="ISSNe">2475-9066</idno>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page">861</biblScope>
			<date type="published" when="2018-09-02" />
			<publisher>The Open Journal</publisher>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Constructing Distributed Representations Using Additive Clustering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/1120.003.0018</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 14</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature Extraction Using an Unsupervised Neural Network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7011.003.0012</idno>
	</analytic>
	<monogr>
		<title level="m">Unsupervised Learning</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-dynamic Bayesian Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.003.0056</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hyperparameter Learning for Graph Based Semi-supervised Learning Algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Siddharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/7503.003.0203</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 19</title>
				<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Separating Style and Content with Bilinear Models</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">10.1162/089976600300015349</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<title level="j" type="abbrev">Neural Computation</title>
		<idno type="ISSN">0899-7667</idno>
		<idno type="ISSNe">1530-888X</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000-06-01" />
			<publisher>MIT Press - Journals</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Learning Identity-Preserving Face Space</title>
		<author>
			<persName><forename type="first">Zhenyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2013.21</idno>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
